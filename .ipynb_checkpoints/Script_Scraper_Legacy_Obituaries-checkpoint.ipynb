{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import time \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Script to check whether the institution is mentioned in the obituary and extract date of birth and date of death\n",
    "# Choose University ID\n",
    "#List of Instititions:\n",
    "#Boston University 24\n",
    "#Emory 66\n",
    "#John Hopkins 102\n",
    "#Indiana U 96\n",
    "#Michigan State 132\n",
    "#NYU 146\n",
    "#NWU 153\n",
    "#OSU 156\n",
    "#Oregon State 161 \n",
    "#Penn State 163 \n",
    "#Princeton\n",
    "#Purdue \n",
    "#Rutgers 177\n",
    "#Stony Brook\n",
    "#U Arizona\n",
    "#UT Austin 229 \n",
    "# UCSD 256 \n",
    "#U of Cincinnati\n",
    "#U of Colorado, Boulder 263\n",
    "#UIUC 280 \n",
    "#U Iowa 281\n",
    "#U Kansas 282 \n",
    "#U Michigan \n",
    "#U Oregon 325 \n",
    "#U Penn 326 \n",
    "#U Pittsburgh 327\n",
    "#U Virginia 345\n",
    "#UW Madison 347 \n",
    "#WUSTL 358 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "\n",
    "# Connect to the sql server\n",
    "cnx = mysql.connector.connect(user='widr', password='nnamd28',\n",
    "                              host='10.0.46.2',\n",
    "                              database='rainer')\n",
    "\n",
    "# Get attribute of cursor\n",
    "mycursor = cnx.cursor()\n",
    "\n",
    "# Select the table\n",
    "mycursor.execute(\"SELECT * FROM umetrics_faculty_last_entry_2009_2016\")\n",
    "\n",
    "# Extract the names of the columns from the sql database\n",
    "sequence = mycursor.column_names\n",
    "\n",
    "# Create a list with all the information\n",
    "myresult = mycursor.fetchall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct dataframe using name_columns\n",
    "all_prof = pd.DataFrame(myresult, columns=sequence)\n",
    "all_prof.head()\n",
    "for col in all_prof.columns: \n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check ducplicates\n",
    "all_prof.duplicated(['PersonName'])\n",
    "\n",
    "# Rename columns which contain person´s name and university´s name and IDs\n",
    "all_prof = all_prof.rename(columns={\"PersonName\": \"name\",\"PersonId\": \"person_id\" , \\\n",
    "                                    \"Year\": \"year\", \"InstitutionName.x\": \"university\", \"InstitutionId\": \"univ_id\",\\\n",
    "                                   \"FirstName\": \"firstname\",\"LastName\": \"lastname\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in all_prof.head().iterrows():\n",
    "     # access data using column names\n",
    "     print(index, row['firstname'], row['Nickname'], row['lastname'], row['university'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort from the latest year and name \n",
    "# This is important since we take only the last appereance of the professor in the University (but you have done it in sql before)\n",
    "# Then afet when we get the date from genealogy we can compare how many years have passed until his death\n",
    "all_prof = all_prof.sort_values(by=['year', 'name'], ascending=False)\n",
    "\n",
    "# Eliminate the duplicates by preserving the first observation \n",
    "all_prof = all_prof.drop_duplicates(subset=['name', 'university', 'year'], keep = 'first')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WE DONT HAVE INFORMATION OF YEAR OF BIRTH NIETHER AGE \n",
    "\n",
    "# now we will keep just information before 2014 since in the genealogy webpage we have no information after that period\n",
    "# This was done from you before\n",
    "\n",
    "#  This is the line we need to change to include all  data \n",
    "# all_prof = all_prof[all_prof[\"year\"] <= 2014]\n",
    "\n",
    "# resort for looking first from 2009-2014 \n",
    "all_prof = all_prof.sort_values(by=['year', 'name'], ascending=True)\n",
    "all_prof[\"year\"].value_counts()\n",
    "all_prof[\"univ_id\"].value_counts()\n",
    "all_prof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create range for date of death \n",
    "#all_prof.rename(columns={'year' : 'death_1'}, inplace=True)\n",
    "\n",
    "all_prof['death_1'] = '01/01/2014'\n",
    "all_prof['death_2'] = '06/30/2019'\n",
    "\n",
    "\n",
    "for index, row in all_prof.head().iterrows():\n",
    "     # access data using column names\n",
    "     print(index, row['name'], row['death_1'], row['death_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to change the type of input \n",
    "\n",
    "all_prof['death_1'] = all_prof['death_1'].astype('str')\n",
    "all_prof['death_2'] = all_prof['death_2'].astype('str')\n",
    "\n",
    "all_prof['death_1'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_prof['title'] = \"Professor\"\n",
    "all_prof['death_2'].head()\n",
    "all_prof['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_prof['university'].value_counts\n",
    "#all_prof['university'].str.findall('Purdue')\n",
    "\n",
    "all_prof[all_prof['university'].str.match('Ohio')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choose University ID ,  For instance  \"Boston = 24\" \n",
    "\n",
    "univ = 156\n",
    "univ_prof = all_prof[all_prof['univ_id']== univ]\n",
    "univ_prof\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_prof = all_prof.sort_values(by=['death_1'], ascending=False)\n",
    "\n",
    "# Create table to fill up with the information\n",
    "columns_names = ['Name_web', 'Date_Death']\n",
    "\n",
    "# scrap_data will contain information from the webpage\n",
    "scrap_data_1 = pd.DataFrame(columns=columns_names)\n",
    "scrap_data_1 = scrap_data_1.fillna(0)\n",
    "\n",
    "\n",
    "# Create table to fill up with the information\n",
    "\n",
    "# scrap_data will contain information from the webpage\n",
    "scrap_data_2 = pd.DataFrame(columns=columns_names)\n",
    "scrap_data_2 = scrap_data_2.fillna(0)\n",
    "\n",
    "\n",
    "# Create text file where we save all information\n",
    "f = open(\"test_1.txt\",\"w+\")\n",
    "g = open(\"test_2.txt\",\"w+\")\n",
    "\n",
    "# Code to find people in genealogybank\n",
    "driver = webdriver.Chrome('C:/Users/Alexander/Documents/Chrome_Driver/chromedriver.exe')\n",
    "driver.get('http://www.legacy.com/obituaries/legacy/obituary-search.aspx?daterange=30&countryid=0&stateid=all&affiliateid=all')\n",
    "\n",
    "# Close advertisement\n",
    "driver.find_element_by_xpath('//*[@id=\"qcCmpButtons\"]/button').click()\n",
    "\n",
    "# Maximize the window\n",
    "driver.maximize_window()\n",
    "\n",
    " # Set the state of last residence\n",
    "driver.find_element_by_xpath('//*[@id=\"ctl00_ctl00_ContentPlaceHolder1_ContentPlaceHolder1_uxSearchWideControl_ddlCountry\"]/option[11]').click()\n",
    "\n",
    "\n",
    "for index, row in tqdm(univ_prof.iterrows()):\n",
    "    try:\n",
    "        # Set the state of last residence\n",
    "        driver.find_element_by_xpath('//*[@id=\"ctl00_ctl00_ContentPlaceHolder1_ContentPlaceHolder1_uxSearchWideControl_ddlCountry\"]/option[11]').click()\n",
    "\n",
    "\n",
    "        # range of death\n",
    "        driver.find_element_by_xpath('//*[@id=\"ctl00_ctl00_ContentPlaceHolder1_ContentPlaceHolder1_uxSearchWideControl_ddlSearchRange\"]/option[10]').click()\n",
    "\n",
    "        death_begin = driver.find_element_by_xpath('//*[@id=\"ctl00_ctl00_ContentPlaceHolder1_ContentPlaceHolder1_uxSearchWideControl_txtStartDate\"]')\n",
    "        death_begin.send_keys(row['death_1'])\n",
    "\n",
    "        death_end = driver.find_element_by_xpath('//*[@id=\"ctl00_ctl00_ContentPlaceHolder1_ContentPlaceHolder1_uxSearchWideControl_txtEndDate\"]')    \n",
    "        death_end.send_keys(row['death_2'])\n",
    "\n",
    "        # type the Firstname \n",
    "        keyword = driver.find_element_by_xpath('//*[@id=\"ctl00_ctl00_ContentPlaceHolder1_ContentPlaceHolder1_uxSearchWideControl_txtFirstName\"]')\n",
    "        keyword.send_keys(row['firstname'])\n",
    "\n",
    "        # type the Lastname \n",
    "        keyword = driver.find_element_by_xpath('//*[@id=\"ctl00_ctl00_ContentPlaceHolder1_ContentPlaceHolder1_uxSearchWideControl_txtLastName\"]')\n",
    "        keyword.send_keys(row['lastname'])\n",
    "\n",
    "        # type the Title \n",
    "        keyword = driver.find_element_by_xpath('//*[@id=\"ctl00_ctl00_ContentPlaceHolder1_ContentPlaceHolder1_uxSearchWideControl_txtKeyword\"]')\n",
    "        keyword.send_keys(row['title'])\n",
    "\n",
    "        # Send information\n",
    "        driver.find_element_by_xpath('//*[@id=\"lnkSearch\"]').click()\n",
    "\n",
    "        # Display restults \n",
    "        try:\n",
    "            found = driver.find_element_by_xpath('//*[@id=\"ctl00_ctl00_ContentPlaceHolder1_ContentPlaceHolder1_uxSearchLinks_Message\"]')\n",
    "\n",
    "        except: \n",
    "            pass\n",
    "\n",
    "        try:        \n",
    "            found = driver.find_element_by_xpath('//*[@id=\"ctl00_ctl00_ContentPlaceHolder1_ContentPlaceHolder1_InlineTotalCount\"]/div')\n",
    "            found_1 = str(found.text)\n",
    "            num_found = int(re.search(r'\\d+', found_1).group(0))\n",
    "            iterator = 3*num_found-1\n",
    "            print(num_found)\n",
    "\n",
    "        except:\n",
    "            num_found = 0\n",
    "            #pass\n",
    "\n",
    "        print(found.text)\n",
    "        time.sleep(3)\n",
    "\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\") \n",
    "        time.sleep(2)\n",
    "\n",
    "\n",
    "        try:      \n",
    "            for i in range(1, iterator, 3):\n",
    "                #time.sleep(3)\n",
    "                try:\n",
    "                    info_1 = driver.find_element_by_xpath('//*[@id=\"scrollpage1\"]/div[%s]/div[2]'%(i))\n",
    "                    information_1 = str(info_1.text)\n",
    "\n",
    "                    real_name = str(row['name'])\n",
    "                    university = str(row['university'])\n",
    "                    univ_id = str(row['univ_id'])\n",
    "                    person_id = str(row['person_id'])\n",
    "\n",
    "                    data_1 = pd.DataFrame({ 'Univ_Id':[univ_id],'University':[university],'Person_Id':[person_id], \\\n",
    "                                           'Real_Name':[real_name],'Num_people':[num_found], 'Information_1':[information_1]})\n",
    "\n",
    "                     # Get elements\n",
    "                    data_1['Name_web'] = str(driver.find_element_by_xpath('//*[@id=\"scrollpage1\"]/div[%s]/div[2]/div[1]/div[1]'%(i)).text)\n",
    "                    # data_2['Date_Death'] = str(driver.find_element_by_xpath('//*[@id=\"scrollpage1\"]/div[%s]/div[2]/div[1]/div[1]'%(i)).text)\n",
    "\n",
    "\n",
    "                    #Fill up the text file with every person we found\n",
    "                    with open(\"test_1.txt\", \"a\") as myfile:\n",
    "                        myfile.write( info_1.text + \"\\n\")\n",
    "\n",
    "                    #print(i)munich \n",
    "                    print(info_1.text)\n",
    "\n",
    "                    # We append information extracted from data_1\n",
    "                    scrap_data_1 = scrap_data_1.append(data_1)\n",
    "\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "\n",
    "            for a in range(1, iterator, 3):\n",
    "                time.sleep(3)\n",
    "\n",
    "                data_2 = pd.DataFrame({ 'Univ_Id':[univ_id],'University':[university],'Person_Id':[person_id], \\\n",
    "                                           'Real_Name':[real_name],'Num_people':[num_found]})\n",
    "\n",
    "                data_2['Name_web'] = str(driver.find_element_by_xpath('//*[@id=\"scrollpage1\"]/div[%s]/div[2]/div[1]/div[1]'%(a)).text)\n",
    "\n",
    "\n",
    "                driver.find_element_by_xpath('//*[@id=\"scrollpage1\"]/div[%s]'%(a)).click()\n",
    "\n",
    "                try :\n",
    "                    time.sleep(3)\n",
    "                    info_2 = driver.find_element_by_xpath('//*[@id=\"ctl00_MainContentPlaceholder_ObitTextContainer\"]')\n",
    "                    information_2 = str(info_2.text)            \n",
    "\n",
    "                except :\n",
    "                    info_2 = \"THIS PERSON IS IN ANOTHER WEBPAGE\"\n",
    "                    information_2 = str(info_2)\n",
    "                    print(\"THIS PERSON IS IN ANOTHER WEBPAGE : \" + str(row['name']))\n",
    "\n",
    "                finally :\n",
    "                    data_2['Information_2'] = information_2\n",
    "\n",
    "                    # Fill up the text file with every person we found\n",
    "                    with open(\"test_2.txt\", \"a\") as myfile:\n",
    "                        myfile.write(information_2 + \"\\n\")\n",
    "\n",
    "                    # We append information extracted from data_2\n",
    "                    scrap_data_2 = scrap_data_2.append(data_2)\n",
    "\n",
    "                    print(\"BIOGRAPHY-\" + information_2)\n",
    "                    driver.back()\n",
    "\n",
    "            driver.get('http://www.legacy.com/obituaries/legacy/obituary-search.aspx?daterange=30&countryid=0&stateid=all&affiliateid=all')        \n",
    "\n",
    "        except:\n",
    "            print(\"THIS PERSON HAS NOT BEEN FOUND : \" + str(row['name']))\n",
    "            # come back original webpage\n",
    "            driver.get('http://www.legacy.com/obituaries/legacy/obituary-search.aspx?daterange=30&countryid=0&stateid=all&affiliateid=all')        \n",
    "\n",
    "    except:\n",
    "        driver.close()\n",
    "        # Code to find people in genealogybank\n",
    "        driver = webdriver.Chrome('C:/Users/quia/Desktop/chromedriver.exe')\n",
    "        driver.get('http://www.legacy.com/obituaries/legacy/obituary-search.aspx?daterange=30&countryid=0&stateid=all&affiliateid=all')\n",
    "\n",
    "        # Close advertisement\n",
    "        driver.find_element_by_xpath('//*[@id=\"qcCmpButtons\"]/button').click()\n",
    "\n",
    "        # Maximize the window\n",
    "        driver.maximize_window()\n",
    "\n",
    "         # Set the state of last residence\n",
    "        driver.find_element_by_xpath('//*[@id=\"ctl00_ctl00_ContentPlaceHolder1_ContentPlaceHolder1_uxSearchWideControl_ddlCountry\"]/option[11]').click()\n",
    "\n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrap_data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scrap_data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This lines eliminate new line´s creation \"\\n\"\n",
    "\n",
    "scrap_data_1[\"Information_1\"] = scrap_data_1[\"Information_1\"].str.replace('\\n',' ', regex=True) \n",
    "scrap_data_2[\"Information_2\"] = scrap_data_2[\"Information_2\"].str.replace('\\n',' ', regex=True) \n",
    "# scrap_data_2.iloc[50,1]#.replace(\"\\n\",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to save the file \n",
    "scrap_data_1.to_csv('C:/Users/quia/Desktop/Faculty_Rosters/scrap_data_1_ohio_state.csv', encoding='utf-8', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to save the file \n",
    "scrap_data_2.to_csv('C:/Users/quia/Desktop/Faculty_Rosters/scrap_data_2_ohio_state.csv', encoding='utf-8',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
